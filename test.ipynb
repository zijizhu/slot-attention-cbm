{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from timm import create_model, list_models\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = create_model('resnet18', pretrained=True)\n",
    "conv1x1 = nn.Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, tensor_list):\n",
    "        x = tensor_list\n",
    "        b, c, h, w = x.shape\n",
    "        mask = torch.zeros((b, h, w), dtype=torch.bool, device=x.device)\n",
    "        not_mask = ~mask\n",
    "        # y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
    "        # x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
    "\n",
    "        x_embed = torch.arange(w).unsqueeze(0).repeat(h, 1).unsqueeze(0).repeat(b, 1, 1)\n",
    "        y_embed = torch.arange(w).unsqueeze(1).repeat(1, h).unsqueeze(0).repeat(b, 1, 1)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "\n",
    "\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(sin_inp):\n",
    "    \"\"\"\n",
    "    Gets a base embedding for one dimension with sin and cos intertwined\n",
    "    \"\"\"\n",
    "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
    "    return torch.flatten(emb, -2, -1)\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / 4) * 2)\n",
    "        self.channels = channels\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.register_buffer(\"cached_penc\", None, persistent=False)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "\n",
    "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
    "            return self.cached_penc\n",
    "\n",
    "        self.cached_penc = None\n",
    "        batch_size, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "        pos_y = torch.arange(y, device=tensor.device, dtype=self.inv_freq.dtype)\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x).unsqueeze(1)\n",
    "        emb_y = get_emb(sin_inp_y)\n",
    "        emb = torch.zeros(\n",
    "            (x, y, self.channels * 2),\n",
    "            device=tensor.device,\n",
    "            dtype=tensor.dtype,\n",
    "        )\n",
    "        emb[:, :, : self.channels] = emb_x\n",
    "        emb[:, :, self.channels : 2 * self.channels] = emb_y\n",
    "\n",
    "        self.cached_penc = emb[None, :, :, :orig_ch].repeat(tensor.shape[0], 1, 1, 1)\n",
    "        return self.cached_penc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 1, 128).expand(-1,10,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 128]), torch.Size([1, 10, 128]))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slots_mu = nn.Parameter(torch.randn(1, 1, 128))\n",
    "slots_sigma = nn.Parameter(torch.abs(torch.randn(1, 1, 128)))\n",
    "mu = slots_mu.expand(1, 10, -1)\n",
    "sigma = slots_sigma.expand(1, 10, -1)\n",
    "mu.shape, sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_slots = nn.Parameter(torch.normal(mu, sigma))\n",
    "initial_slots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScouterAttention(nn.Module):\n",
    "    def __init__(self, dim, num_concept, iters=3, eps=1e-8, vis=False, power=1, to_k_layer=3):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_concept\n",
    "        self.iters = iters\n",
    "        self.eps = eps\n",
    "        self.scale = dim ** (-0.5)\n",
    "\n",
    "        # random seed init\n",
    "        slots_mu = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        slots_sigma = nn.Parameter(torch.abs(torch.randn(1, 1, dim)))\n",
    "        mu = slots_mu.expand(1, self.num_slots, -1)\n",
    "        sigma = slots_sigma.expand(1, self.num_slots, -1)\n",
    "        self.initial_slots = nn.Parameter(torch.normal(mu, sigma))\n",
    "\n",
    "        # K layer init\n",
    "        to_k_layer_list = [nn.Linear(dim, dim)]\n",
    "        for to_k_layer_id in range(1, to_k_layer):\n",
    "            to_k_layer_list.append(nn.ReLU(inplace=True))\n",
    "            to_k_layer_list.append(nn.Linear(dim, dim))\n",
    "        self.to_k = nn.Sequential(\n",
    "            *to_k_layer_list\n",
    "        )\n",
    "\n",
    "        self.vis = vis\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, inputs_pe, inputs):\n",
    "        b, n, d = inputs_pe.shape\n",
    "        slots = self.initial_slots.expand(b, -1, -1)\n",
    "        k, v = self.to_k(inputs_pe), inputs_pe\n",
    "        print('slot.k.shape:', k.shape)\n",
    "        for _ in range(self.iters):\n",
    "            q = slots\n",
    "            print('slot.q.shape:', q.shape)\n",
    "\n",
    "            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale\n",
    "            dots = torch.div(\n",
    "                dots,\n",
    "                (dots.sum(2)\n",
    "                 .expand_as(dots.permute([2, 0, 1]))\n",
    "                 .permute([1, 2, 0]))\n",
    "                 * dots.sum(2).sum(1).expand_as(dots.permute([1, 2, 0]))\n",
    "                    .permute([2, 0, 1]))\n",
    "            attn = torch.sigmoid(dots)\n",
    "            print('slot.dots.shape:', dots.shape)\n",
    "            print('slot.attn.shape:', attn.shape)\n",
    "\n",
    "            attn2 = attn / (attn.sum(dim=-1, keepdim=True) + self.eps)\n",
    "            print('slot.attn2.shape:', attn2.shape)\n",
    "            updates = torch.einsum('bjd,bij->bid', inputs, attn2)\n",
    "            print('slot.updates.shape:', updates.shape)\n",
    "            break\n",
    "        return updates, attn\n",
    "\n",
    "\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, args, vis=False):\n",
    "        super(MainModel, self).__init__()\n",
    "        self.args = args\n",
    "        self.pre_train = args.pre_train\n",
    "        if \"18\" not in args.base_model:\n",
    "            self.num_features = 2048\n",
    "        else:\n",
    "            self.num_features = 512\n",
    "        self.feature_size = args.feature_size\n",
    "        self.drop_rate = 0.0\n",
    "        hidden_dim = 128\n",
    "        num_concepts = args.num_cpt\n",
    "        num_classes = args.num_classes\n",
    "        self.back_bone = create_model('resnet18', pretrained=True)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.vis = vis\n",
    "\n",
    "        if not self.pre_train:\n",
    "            self.conv1x1 = nn.Conv2d(self.num_features, hidden_dim, kernel_size=(1, 1), stride=(1, 1))\n",
    "            self.norm = nn.BatchNorm2d(hidden_dim)\n",
    "            self.position_emb = PositionEmbeddingSine(hidden_dim // 2, normalize=True)\n",
    "            self.slots = ScouterAttention(hidden_dim, num_concepts, vis=self.vis)\n",
    "            self.cls = torch.nn.Linear(num_concepts, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.num_features, num_classes)\n",
    "            self.drop_rate = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.back_bone.forward_features(x)\n",
    "        features = x\n",
    "        # x = x.view(x.size(0), self.num_features, self.feature_size, self.feature_size)\n",
    "\n",
    "        if not self.pre_train:\n",
    "            x = self.conv1x1(x)\n",
    "            x = self.norm(x)\n",
    "            x = torch.relu(x)\n",
    "            pe = self.position_emb(x)\n",
    "            x_pe = x + pe\n",
    "\n",
    "            b, c, h ,w = x.shape\n",
    "            x = x.reshape((b, c, -1)).permute((0, 2, 1)) # shape: b, c, h*w\n",
    "            x_pe = x_pe.reshape((b, c, -1)).permute((0, 2, 1)) # shape: b, c, h*w\n",
    "            print('x.shape:', x.shape, 'x_pe.shape:', x_pe.shape)\n",
    "\n",
    "            updates, attn = self.slots(x_pe, x)\n",
    "            if self.args.cpt_activation == \"att\":\n",
    "                cpt_activation = attn\n",
    "            else:\n",
    "                cpt_activation = updates\n",
    "            attn_cls = self.scale * torch.sum(cpt_activation, dim=-1)\n",
    "            cpt = self.activation(attn_cls)\n",
    "            cls = self.cls(cpt)\n",
    "            return (cpt - 0.5) * 2, cls, attn, updates\n",
    "        else:\n",
    "            x = F.adaptive_max_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "            if self.drop_rate > 0:\n",
    "                x = F.dropout(x, p=self.drop_rate, training=self.training)\n",
    "            x = self.fc(x)\n",
    "            return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self) -> None:\n",
    "        self.pre_train = False\n",
    "        self.base_model = 'resnet18'\n",
    "        self.feature_size = 7\n",
    "        self.num_cpt = 15\n",
    "        self.num_classes = 200\n",
    "        self.cpt_activation = 'att'\n",
    "\n",
    "args = Arguments()\n",
    "model = MainModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([32, 49, 128]) x_pe.shape: torch.Size([32, 49, 128])\n",
      "slot.k.shape: torch.Size([32, 49, 128])\n",
      "slot.q.shape: torch.Size([32, 15, 128])\n",
      "slot.dots.shape: torch.Size([32, 15, 49])\n",
      "slot.attn.shape: torch.Size([32, 15, 49])\n",
      "slot.attn2.shape: torch.Size([32, 15, 49])\n",
      "slot.updates.shape: torch.Size([32, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "a, b, c, d = model(torch.randn(32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.rand([32, 15, 49])\n",
    "a = dots.sum(2).expand_as(dots.permute([2, 0, 1])).permute([1, 2, 0]) * dots.sum(2).sum(1).expand_as(dots.permute([1, 2, 0])).permute([2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 15, 49])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
